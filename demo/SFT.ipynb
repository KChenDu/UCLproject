{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Supervised Fine-tuning Trainer\n",
    "Supervised fine-tuning (or SFT for short) is a crucial step in RLHF. In TRL we provide an easy-to-use API to create your SFT models and train them with few lines of code on your dataset.\n",
    "\n",
    "Check out a complete flexible example at [`examples/scripts/sft.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/sft.py). Experimental support for Vision Language Models is also included in the [`example examples/scripts/vsft_llava.py`](https://github.com/huggingface/trl/tree/main/examples/scripts/vsft_llava.py).\n",
    "## Quickstart\n",
    "If you have a dataset hosted on the Hub, you can easily fine-tune your SFT model using [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer) from TRL. Let us assume your dataset is imdb, the text you want to predict is inside the text field of the dataset, and you want to fine-tune the `facebook/opt-350m` model. The following code-snippet takes care of all the data pre-processing and training for you:"
   ],
   "id": "ed3bed7e5abe1e9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "sft_config = SFTConfig(dataset_text_field=\"text\", max_seq_length=512, output_dir=\"/tmp\")\n",
    "trainer = SFTTrainer(\"facebook/opt-350m\", train_dataset=dataset, args=sft_config)\n",
    "trainer.train()"
   ],
   "id": "11d2a1ba8e0ac430",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Make sure to pass the correct value for `max_seq_length` as the default value will be set to `min`(`tokenizer.model_max_length`, `1024`).\n",
    "\n",
    "You can also construct a model outside of the trainer and pass it as follows:"
   ],
   "id": "a32f0b7d4678e7c3"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "sft_config = SFTConfig(output_dir=\"/tmp\")\n",
    "\n",
    "trainer = SFTTrainer(model, train_dataset=dataset, args=sft_config)\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The above snippets will use the default training arguments from the [SFTConfig](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTConfig) class. If you want to modify the defaults pass in your modification to the `SFTConfig` constructor and pass them to the trainer via the args argument.",
   "id": "daed118996616247"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Advanced usage\n",
    "### Train on completions only\n",
    "You can use the `DataCollatorForCompletionOnlyLM` to train your model on the generated prompts only. Note that this works only in the case when `packing=False`. To instantiate that collator for instruction data, pass a response template and the tokenizer. Here is an example of how it would work to fine-tune `opt-350m` on completions only on the CodeAlpaca dataset:"
   ],
   "id": "c3c0b191450ea2b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"lucasmccabe-lmi/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(model, train_dataset=dataset, args=SFTConfig(output_dir=\"/tmp\"), formatting_func=formatting_prompts_func, data_collator=collator)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "bb265628efe03764",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To instantiate that collator for assistant style conversation data, pass a response template, an instruction template and the tokenizer. Here is an example of how it would work to fine-tune `opt-350m` on assistant completions only on the Open Assistant Guanaco dataset:",
   "id": "10ea44ae468a3f07"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "instruction_template = \"### Human:\"\n",
    "response_template = \"### Assistant:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = SFTTrainer(model, args=SFTConfig(output_dir=\"/tmp\"), train_dataset=dataset, data_collator=collator)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "ecfe9114c099906e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Make sure to have a `pad_token_id` which is different from eos_token_id which can result in the model not properly predicting EOS (End of Sentence) tokens during generation.\n",
    "### Using token_ids directly for response_template\n",
    "Some tokenizers like Llama 2 (`meta-llama/Llama-2-XXb-hf`) tokenize sequences differently depending on whether they have context or not. For example:"
   ],
   "id": "6619e98845d954dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "\n",
    "prompt = \"\"\"### User: Hello\\n\\n### Assistant: Hi, how can I help you?\"\"\"\n",
    "print_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n",
    "\n",
    "response_template = \"### Assistant:\"\n",
    "print_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]"
   ],
   "id": "86603f8ab94864e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this case, and due to lack of context in `response_template`, the same string (”### Assistant:”) is tokenized differently:\n",
    "\n",
    "- Text (with context): [2277, 29937, 4007, 22137, 29901]\n",
    "- `response_template` (without context): [835, 4007, 22137, 29901]\n",
    "\n",
    "This will lead to an error when the DataCollatorForCompletionOnlyLM does not find the response_template in the dataset example text:"
   ],
   "id": "928bcabdb75a4b09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To solve this, you can tokenize the `response_template` with the same context as in the dataset, truncate it as needed and pass the `token_ids` directly to the `response_template` argument of the `DataCollatorForCompletionOnlyLM` class. For example:",
   "id": "862d698de77657e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response_template_with_context = \"\\n### Assistant:\"  # We added context here: \"\\n\". This is enough for this tokenizer\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[2:]  # Now we have it like in the dataset texts: `[2277, 29937, 4007, 22137, 29901]`\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ],
   "id": "abd804fec6aa2889",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Add Special Tokens for Chat Format\n",
    "Adding special tokens to a language model is crucial for training chat models. These tokens are added between the different roles in a conversation, such as the user, assistant, and system and help the model recognize the structure and flow of a conversation. This setup is essential for enabling the model to generate coherent and contextually appropriate responses in a chat environment. The `setup_chat_format()` function in `trl` easily sets up a model and tokenizer for conversational AI tasks. This function:\n",
    "\n",
    "- Adds special tokens to the tokenizer, e.g. `<|im_start|>` and `<|im_end|>`, to indicate the start and end of a conversation.\n",
    "- Resizes the model’s embedding layer to accommodate the new tokens.\n",
    "- Sets the `chat_template` of the tokenizer, which is used to format the input data into a chat-like format. The default is `chatml` from OpenAI.\n",
    "- `optionally` you can pass `resize_to_multiple_of` to resize the embedding layer to a multiple of the `resize_to_multiple_of` argument, e.g. 64. If you want to see more formats being supported in the future, please open a GitHub issue on [trl](https://github.com/huggingface/trl)"
   ],
   "id": "35065296dbfbd5f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from trl import setup_chat_format\n",
    "\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
    "\n",
    "# Set up the chat format with default 'chatml' format\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ],
   "id": "d67b3687bdf409f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With our model and tokenizer set up, we can now fine-tune our model on a conversational dataset. Below is an example of how a dataset can be formatted for fine-tuning.",
   "id": "de5974ec5aa8aba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset format support\n",
    "The [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer) supports popular dataset formats. This allows you to pass the dataset to the trainer without any pre-processing directly. The following formats are supported:\n",
    "\n",
    "- conversational format"
   ],
   "id": "f5b012cd6a93cc6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are helpful\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"...\"}]}"
   ],
   "id": "519158dc7616bbcf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- instruction format",
   "id": "c3517bf1d9f843f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}\n",
    "{\"prompt\": \"<prompt text>\", \"completion\": \"<ideal generated text>\"}"
   ],
   "id": "61351b0b26bb1320",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If your dataset uses one of the above formats, you can directly pass it to the trainer without pre-processing. The [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer) will then format the dataset for you using the defined format from the model’s tokenizer with the [`apply_chat_template`](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models) method.\n",
    "\n",
    "If the dataset is not in one of those format you can either preprocess the dataset to match the formatting or pass a formatting function to the SFTTrainer to do it for you.\n",
    "### Customize your prompts using packed dataset\n",
    "If your dataset has several fields that you want to combine, for example if the dataset has `question` and `answer` fields and you want to combine them, you can pass a formatting function to the trainer that will take care of that. For example:"
   ],
   "id": "cdac17964278b1e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def formatting_func(example):\n",
    "    text = f\"### Question: {example['question']}\\n ### Answer: {example['answer']}\"\n",
    "    return text\n",
    "\n",
    "\n",
    "sft_config = SFTConfig(packing=True)\n",
    "trainer = SFTTrainer(\"facebook/opt-350m\", train_dataset=dataset, args=sft_config, formatting_func=formatting_func)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "ed4c9029dd2f0d04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can also customize the `ConstantLengthDataset` much more by directly passing the arguments to the [`SFTConfig`](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTConfig) constructor. Please refer to that class’ signature for more information.\n",
    "### Training adapters\n",
    "We also support tight integration with PEFT library so that any user can conveniently train adapters and share them on the Hub instead of training the entire model"
   ],
   "id": "e53016280b020a09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\")\n",
    "\n",
    "trainer = SFTTrainer(\"EleutherAI/gpt-neo-125m\", train_dataset=dataset, args=SFTConfig(output_dir=\"/tmp\"), peft_config=peft_config)\n",
    "\n",
    "trainer.train()"
   ],
   "id": "bee702fc94714150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can also continue training your `PeftModel`. For that, first load a PeftModel outside `SFTTrainer` and pass it directly to the trainer without the `peft_config` argument being passed.\n",
    "### Enhance the model’s performances using NEFTune\n",
    "NEFTune is a technique to boost the performance of chat models and was introduced by the paper “[NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)” from Jain et al. it consists of adding noise to the embedding vectors during training. According to the abstract of the paper:\n",
    "\n",
    "\"Standard finetuning of LLaMA-2-7B using Alpaca achieves 29.79% on AlpacaEval, which rises to 64.69% using noisy embeddings. NEFTune also improves over strong baselines on modern instruction datasets. Models trained with Evol-Instruct see a 10% improvement, with ShareGPT an 8% improvement, and with OpenPlatypus an 8% improvement. Even powerful models further refined with RLHF such as LLaMA-2-Chat benefit from additional training with NEFTune.\"\n",
    "\n",
    "To use it in `SFTTrainer` simply pass `neftune_noise_alpha` when creating your `SFTConfig` instance. Note that to avoid any surprising behaviour, NEFTune is disabled after training to retrieve back the original behaviour of the embedding layer."
   ],
   "id": "f7d31b0ebc31a34b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sft_config = SFTConfig(neftune_noise_alpha=5,)\n",
    "trainer = SFTTrainer(\"facebook/opt-350m\", train_dataset=dataset, args=sft_config)\n",
    "trainer.train()"
   ],
   "id": "e8cceb1d38b0d55a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have tested NEFTune by training `mistralai/Mistral-7B-v0.1` on the [OpenAssistant](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset and validated that using NEFTune led to a performance boost of ~25% on MT Bench.\n",
    "\n",
    "Note however, that the amount of performance gain is dataset dependent and in particular, applying NEFTune on synthetic datasets like [UltraChat](https://huggingface.co/datasets/stingning/ultrachat) typically produces smaller gains.\n",
    "### Accelerate fine-tuning 2x using unsloth\n",
    "You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using the unsloth library that is fully compatible with SFTTrainer. Currently unsloth supports only Llama (Yi, TinyLlama, Qwen, Deepseek etc) and Mistral architectures.\n",
    "\n",
    "First install unsloth according to the official documentation. Once installed, you can incorporate unsloth into your workflow in a very simple manner; instead of loading AutoModelForCausalLM, you just need to load a FastLanguageModel as follows:"
   ],
   "id": "db9bb7bd3d514ef7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/mistral-7b\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit=True,  # Use 4bit quantization to reduce memory usage. Can be False\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Dropout = 0 is currently optimized\n",
    "    bias=\"none\",  # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing=True,\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "args = SFTConfig(output_dir=\"./output\", max_seq_length=max_seq_length, dataset_text_field=\"text\")\n",
    "\n",
    "trainer = SFTTrainer(model=model, args=args, train_dataset=dataset)\n",
    "trainer.train()"
   ],
   "id": "c4ae598f25e8b7fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The saved model is fully compatible with Hugging Face’s transformers library. Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).\n",
    "## Best practices\n",
    "Pay attention to the following best practices when training a model with that trainer:\n",
    "- [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer) always pads by default the sequences to the `max_seq_length` argument of the [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer). If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide a default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.\n",
    "- For training adapters in 8bit, you might need to tweak the arguments of the `prepare_model_for_kbit_training` method from PEFT, hence we advise users to use `prepare_in_int8_kwargs` field, or create the PeftModel outside the [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer) and pass it.\n",
    "- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add `load_in_8bit` argument when creating the [SFTTrainer](https://huggingface.co/docs/trl/v0.9.4/en/sft_trainer#trl.SFTTrainer), or create a base model in 8bit outside the trainer and pass it.\n",
    "- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to `from_pretrained()` method.\n",
    "## Multi-GPU Training\n",
    "Trainer (and thus SFTTrainer) supports multi-GPU training. If you run your script with python script.py it will default to using DP as the strategy, which may be [slower than expected](https://github.com/huggingface/trl/issues/1303). To use DDP (which is generally recommended, see [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_many?select-gpu=Accelerate#data-parallelism) for more info) you must launch the script with `python -m torch.distributed.launch script.py` or `accelerate launch script.py`. For DDP to work you must also check the following:\n",
    "- If you’re using gradient_checkpointing, add the following to the TrainingArguments: `gradient_checkpointing_kwargs={'use_reentrant':False}` (more info here\n",
    "- Ensure that the model is placed on the correct device\n",
    "## GPTQ Conversion\n",
    "You may experience some issues with GPTQ Quantization after completing training. Lowering `gradient_accumulation_steps` to 4 will resolve most issues during the quantization process to GPTQ format."
   ],
   "id": "9392888f6d55f044"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
