{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# DPO Trainer\n",
    "TRL supports the DPO Trainer for training language models from preference data, as described in the paper [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290) by Rafailov et al., 2023. For a full example have a look at [`examples/scripts/dpo.py`](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo.py).\n",
    "\n",
    "The first step as always is to train your SFT model, to ensure the data we train on is in-distribution for the DPO algorithm.\n",
    "\n",
    "## How DPO works\n",
    "Fine-tuning a language model via DPO consists of two steps and is easier than PPO:\n",
    "1. **Data collection**: Gather a preference dataset with positive and negative selected pairs of generation, given a prompt.\n",
    "2. Optimization: Maximize the log-likelihood of the DPO loss directly.\n",
    "\n",
    "DPO-compatible datasets can be found with [the tag dpo on Hugging Face Hub](https://huggingface.co/datasets?other=dpo).\n",
    "\n",
    "Read more about DPO algorithm in the original paper.\n",
    "## Expected dataset format\n",
    "The DPO trainer expects a very specific format for the dataset. Since the model will be trained to directly optimize the preference of which sentence is the most relevant, given two sentences.\n",
    "\n",
    "Therefore the final dataset object should contain these 3 entries if you use the default `DPODataCollatorWithPadding` data collator. The entries should be named:\n",
    "- prompt\n",
    "- chosen\n",
    "- rejected\n",
    "- for example:"
   ],
   "id": "9d06c53a5814d0ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dpo_dataset_dict = {\n",
    "    \"prompt\": [\n",
    "        \"hello\",\n",
    "        \"how are you\",\n",
    "        \"What is your name?\",\n",
    "        \"What is your name?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "        \"Which is the best programming language?\",\n",
    "    ],\n",
    "    \"chosen\": [\n",
    "        \"hi nice to meet you\",\n",
    "        \"I am fine\",\n",
    "        \"My name is Mary\",\n",
    "        \"My name is Mary\",\n",
    "        \"Python\",\n",
    "        \"Python\",\n",
    "        \"Java\",\n",
    "    ],\n",
    "    \"rejected\": [\n",
    "        \"leave me alone\",\n",
    "        \"I am not fine\",\n",
    "        \"Whats it to you?\",\n",
    "        \"I dont have a name\",\n",
    "        \"Javascript\",\n",
    "        \"C++\",\n",
    "        \"C++\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "where the `prompt` contains the context inputs, `chosen` contains the corresponding chosen responses and `rejected` contains the corresponding negative (rejected) responses. As can be seen a prompt can have multiple responses and this is reflected in the entries being repeated in the dictionary’s value arrays.\n",
    "\n",
    "`DPOTrainer` can be used to fine-tune visual language models (VLMs). In this case, the dataset must also contain the key `images`, and the trainer’s `tokenizer` is the VLM’s `processor`. For example, for Idefics2, the processor expects the dataset to have the following format:\n",
    "\n",
    "Note: Currently, VLM support is exclusive to Idefics2 and does not extend to other VLMs."
   ],
   "id": "5deac1efd8de70fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "dpo_dataset_dict = {\n",
    "    'images': [\n",
    "        [Image.open('beach.jpg')],\n",
    "        [Image.open('street.jpg')],\n",
    "    ],\n",
    "    'prompt': [\n",
    "        'The image <image> shows',\n",
    "        '<image> The image depicts',\n",
    "    ],\n",
    "    'chosen': [\n",
    "        'a sunny beach with palm trees.',\n",
    "        'a busy street with several cars and buildings.',\n",
    "    ],\n",
    "    'rejected': [\n",
    "        'a snowy mountain with skiers.',\n",
    "        'a calm countryside with green fields.',\n",
    "    ],\n",
    "}"
   ],
   "id": "a0a9e91e0687c351"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Expected model format\n",
    "The DPO trainer expects a model of `AutoModelForCausalLM` or `AutoModelForVision2Seq`, compared to PPO that expects `AutoModelForCausalLMWithValueHead` for the value function.\n",
    "## Using the DPOTrainer\n",
    "For a detailed example have a look at the `examples/scripts/dpo.py` script. At a high level we need to initialize the `DPOTrainer` with a `model` we wish to train, a reference `ref_model` which we will use to calculate the implicit rewards of the preferred and rejected response, the `beta` refers to the hyperparameter of the implicit reward, and the dataset contains the 3 entries listed above. Note that the `model` and `ref_model` need to have the same architecture (ie decoder only or encoder-decoder)."
   ],
   "id": "8acc154d036f26f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from trl import DPOConfig, DPOTrainer\n",
    "\n",
    "\n",
    "training_args = DPOConfig(\n",
    "    beta=0.1,\n",
    ")\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    model_ref,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,  # for visual language models, use tokenizer=processor instead\n",
    ")"
   ],
   "id": "2c75485112839edf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After this one can then call:",
   "id": "5ff5068cf6204b21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dpo_trainer.train()",
   "id": "73208dd38fab25b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that the `beta` is the temperature parameter for the DPO loss, typically something in the range of $0.1$ to $0.5$. We ignore the reference model as `beta` -> 0.\n",
    "\n",
    "## Loss functions\n",
    "Given the preference data, we can fit a binary classifier according to the Bradley-Terry model and in fact the DPO authors propose the sigmoid loss on the normalized likelihood via the `logsigmoid` to fit a logistic regression.\n",
    "\n",
    "The [RSO](https://arxiv.org/abs/2309.06657) authors propose to use a hinge loss on the normalized likelihood from the [SLiC](https://arxiv.org/abs/2305.10425) paper. The DPOTrainer can be switched to this loss via the `loss_type=\"hinge\"` argument and the beta in this case is the reciprocal of the margin.\n",
    "\n",
    "The [IPO](https://arxiv.org/abs/2310.12036) authors provide a deeper theoretical understanding of the DPO algorithms and identify an issue with overfitting and propose an alternative loss which can be used via the `loss_type=\"ipo\"` argument to the trainer. Note that the beta parameter is the reciprocal of the gap between the log-likelihood ratios of the chosen vs the rejected completion pair and thus the smaller the beta the larger this gaps is. As per the paper the loss is averaged over log-likelihoods of the completion (unlike DPO which is summed only).\n",
    "\n",
    "The [cDPO](https://ericmitchell.ai/cdpo.pdf) is a tweak on the DPO loss where we assume that the preference labels are noisy with some probability that can be passed to the `DPOTrainer` via `label_smoothing` argument (between 0 and 0.5) and then a conservative DPO loss is used. Pass the label_smoothing argument to the trainer to use it (default is 0).\n",
    "\n",
    "The [Robust DPO](https://arxiv.org/abs/2403.00409) authors propose an unbiased estimate of the DPO loss that is robust to preference noise in the data. Like in cDPO, assume that the preference labels are noisy with some probability that can be passed to the `DPOTrainer` via `label_smoothing` argument (between 0 and 0.5). Use `loss_type=\"robust\"` to the trainer to use it.\n",
    "\n",
    "The [EXO](https://arxiv.org/pdf/2402.00856) authors propose to minimize the reverse KL instead of the negative log-sigmoid loss of DPO which corresponds to forward KL. Setting `loss_type=exo_pair` and a non-zero label_smoothing (default `1e-3`) leads to a simplified version of EXO on pair-wise preferences (see Eqn. (16) of the [EXO paper](https://arxiv.org/pdf/2402.00856)). The full version of EXO uses $K > 2$ completions generated by the SFT policy, which becomes an unbiased estimator of the PPO objective (up to a constant) when K is sufficiently large.\n",
    "\n",
    "The [BCO](https://arxiv.org/abs/2404.04656) authors train a binary classifier whose logit serves as a reward so that the classifier maps {prompt, chosen completion} pairs to 1 and {prompt, rejected completion} pairs to 0. The `DPOTrainer` can be switched to this loss via the `loss_type=\"bco_pair\"` argument.\n",
    "\n",
    "The [SPPO](https://arxiv.org/abs/2405.00675) authors claim that SPPO is capable of solving the Nash equilibrium iteratively by pushing the chosen rewards to be as large as 1/2 and the rejected rewards to be as small as -1/2 and can alleviate data sparsity issues. The implementation using loss_type=“sppo_hard” approximates this algorithm by employing hard label probabilities, assigning 1 to the winner and 0 to the loser.\n",
    "\n",
    "The [NCA](https://arxiv.org/abs/2402.05369) authors shows that NCA optimizes the absolute likelihood for each response rather than the relative likelihood.\n",
    "\n",
    "The [TR-DPO](https://arxiv.org/pdf/2404.09656) paper suggests syncing the reference model weights after every `ref_model_sync_steps` steps of SGD with weight `ref_model_mixup_alpha` during DPO training. To toggle this callback use the `sync_ref_model` flag in the `DPOConfig`.\n",
    "\n",
    "The [RPO](https://arxiv.org/abs/2404.19733) paper implements an iterative preference tuning algorithm using a loss related to the RPO loss in this [paper](https://arxiv.org/abs/2405.16436) that essentially consists of the SFT loss on the chosen preferences together with a weighted DPO loss. To use this loss set the `rpo_alpha` in the `DPOConfig` to an appropriate value.\n",
    "\n",
    "The [AOT](https://arxiv.org/abs/2406.05882) authors propose to use Distributional Preference Alignment Via Optimal Transport. Traditionally, the alignment algorithms use paired preferences at a sample level, which does not ensure alignment on the distributional level. AOT, on the other hand, can align LLMs on paired or unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples. Specifically, `loss_type=\"aot\"` is appropriate for paired datasets, where each prompt has both chosen and rejected responses; `loss_type=\"aot_pair\"` is for unpaired datasets. In a nutshell, `loss_type=\"aot\"` ensures that the log-likelihood ratio of chosen to rejected of the aligned model has higher quantiles than that ratio for the reference model. `loss_type=\"aot_pair\"` ensures that the chosen reward is higher on all quantiles than the rejected reward. Note that in both cases quantiles are obtained via sorting. To fully leverage the advantages of the AOT algorithm, it is important to maximize the per-GPU batch size.\n",
    "\n",
    "## For Mixture of Experts Models: Enabling the auxiliary loss\n",
    "MOEs are the most efficient if the load is about equally distributed between experts.\n",
    "\n",
    "To ensure that we train MOEs similarly during preference-tuning, it is beneficial to add the auxiliary loss from the load balancer to the final loss.\n",
    "\n",
    "This option is enabled by setting `output_router_logits=True` in the model config (e.g. MixtralConfig).\n",
    "To scale how much the auxiliary loss contributes to the total loss, use the hyperparameter `router_aux_loss_coef=...` (default: 0.001).\n",
    "\n",
    "## Logging\n",
    "While training and evaluating we record the following reward metrics:\n",
    "- `rewards/chosen`: the mean difference between the log probabilities of the policy model and the reference model for the chosen responses scaled by beta\n",
    "- `rewards/rejected`: the mean difference between the log probabilities of the policy model and the reference model for the rejected responses scaled by beta\n",
    "- `rewards/accuracies`: mean of how often the chosen rewards are > than the corresponding rejected rewards\n",
    "- `rewards/margins`: the mean difference between the chosen and corresponding rejected rewards\n",
    "## Accelerate DPO fine-tuning using unsloth\n",
    "You can further accelerate QLoRA / LoRA (2x faster, 60% less memory) using the [unsloth](https://github.com/unslothai/unsloth) library that is fully compatible with `SFTTrainer`. Currently `unsloth` supports only Llama (Yi, TinyLlama, Qwen, Deepseek etc) and Mistral architectures.\n",
    "\n",
    "First install unsloth according to the [official documentation](https://github.com/unslothai/unsloth). Once installed, you can incorporate unsloth into your workflow in a very simple manner; instead of loading `AutoModelForCausalLM`, you just need to load a `FastLanguageModel` as follows:"
   ],
   "id": "5fe8e875fd242b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "\n",
    "max_seq_length = 2048 # Supports automatic RoPE Scaling, so choose any number.\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/zephyr-sft\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "# Do model patching and add fast LoRA weights\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Dropout = 0 is currently optimized\n",
    "    bias = \"none\",    # Bias = \"none\" is currently optimized\n",
    "    use_gradient_checkpointing = True,\n",
    "    random_state = 3407,\n",
    ")\n",
    "\n",
    "training_args = DPOConfig(output_dir=\"./output\", beta=0.1)\n",
    "\n",
    "dpo_trainer = DPOTrainer(model, ref_model=None, args=training_args, train_dataset=train_dataset, tokenizer=tokenizer)\n",
    "dpo_trainer.train()"
   ],
   "id": "a5f4dd864883a829"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The saved model is fully compatible with Hugging Face’s transformers library. Learn more about unsloth in their [official repository](https://github.com/unslothai/unsloth).",
   "id": "75b023a6295ec7ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
