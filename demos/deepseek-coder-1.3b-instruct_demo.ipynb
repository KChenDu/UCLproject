{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48a78f3300cbf0a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. **Introduction of Deepseek Coder**\n",
    "    \n",
    "    **Deepseek Coder** is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks.\n",
    "    - Massive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages.\n",
    "    - **Highly Flexible & Scalable**: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements.\n",
    "    - **Superior Model Performance**: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks.\n",
    "    - **Advanced Code Completion Capabilities**: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks.\n",
    "2. **Model Summary**\n",
    "    \n",
    "    deepseek-coder-1.3b-base is a 1.3B parameter model with Multi-Head Attention trained on 1 trillion tokens.\n",
    "    - **Home Page**: [DeepSeek](https://www.deepseek.com)\n",
    "    - **Repository**: [deepseek-ai/deepseek-coder](https://github.com/deepseek-ai/deepseek-coder)\n",
    "    - **Chat With DeepSeek Coder**: [DeepSeek-Coder](https://chat.deepseek.com/sign_in?from=coder)\n",
    "3. **How to Use**\n",
    "    \n",
    "    Here give some examples of how to use our model.\n",
    "    **Chat Model Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-04T17:22:29.283771Z",
     "start_time": "2024-04-04T17:22:16.998938Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\kelve\\anaconda3\\envs\\UCLproject\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:32021 for open-end generation.\n",
      "C:\\Users\\kelve\\anaconda3\\envs\\UCLproject\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:728: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here is a simple implementation of the Quick Sort algorithm in Python:\n",
      "\n",
      "```python\n",
      "def quicksort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    pivot = arr[len(arr) // 2]\n",
      "    left = [x for x in arr if x < pivot]\n",
      "    middle = [x for x in arr if x == pivot]\n",
      "    right = [x for x in arr if x > pivot]\n",
      "    return quicksort(left) + middle + quicksort(right)\n",
      "\n",
      "# Test the function\n",
      "print(quicksort([3,6,8,10,1,2,1]))\n",
      "# Output: [1, 1, 2, 3, 6, 8, 10]\n",
      "```\n",
      "\n",
      "This algorithm works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then recursively sorted.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()\n",
    "messages=[\n",
    "    { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "# tokenizer.eos_token_id is the id of <|EOT|> token\n",
    "outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
    "print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d726ae1c6409b67",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Citation\n",
    "```bibtex\n",
    "@misc{deepseek-coder,\n",
    "  author = {Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang},\n",
    "  title = {DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence},\n",
    "  journal = {CoRR},\n",
    "  volume = {abs/2401.14196},\n",
    "  year = {2024},\n",
    "  url = {https://arxiv.org/abs/2401.14196},\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
